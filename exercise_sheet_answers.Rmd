---
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Hidden Libraries
library(knitr)  # For figures, tables and matrices
library(kableExtra)  # For kable matrix
library(pander)  # For more complex matrices symbol 
library(rmarkdown)
library(tidyverse)
```

# Fundamentals of Data Science Assessment Sheet

#### *Matt Prill*

Note: *All code formulated for this assessment is accessible in my github repository linked here. It also contains images of my hand written workings out. My workflow for the R questions can be found in the 'rough_workflow.R' file. There is no onus to look at these; all relevant and neat workings can be seen in this document. I have included them in the repository to **(A)*** track my own workflow (version control) and\* **(B)** *allow you to consult them should you wish to.*

## Question 1a

#### Initial Equations:

$$
\Large
\begin{aligned}
x + y + z &= 1 \\
x + 2y + 4z &= \eta \\
x + 4y + 10z &= \eta^2
\end{aligned}
$$\

#### Corresponding Coefficient Matrix:

$$
\Large
A = \begin{bmatrix}
1 & 1 & 1 \\
1 & 2 & 4 \\
1 & 4 & 10 
\end{bmatrix}
\begin{bmatrix}
1  \\
η  \\
η^2  
\end{bmatrix}
$$

#### Calculating the Determinant of A:

To showcase that these equations lack a unique solution for any value of η, I can attempt to find the determinant. To to find a determinant of square matrix (irrespective of size), I must;

1.  Pick a coefficient from the first row.
2.  Delete remaining elements in coefficient's respective row and column.
3.  Make a matrix of the remaining elements.
4.  Find the determinant of the sub-matrix, and multiply this with the coefficient.
5.  Repeat the same procedure for each element in the first row.
6.  To determine the sign of each term, sum the indices of the coefficient. If it is even, the sign is positive, and if it’s odd, the sign is negative.
7.  Sum all terms to find the determinant (Linear Algebra Slides, 2019).

Therefore, to find the determinant ($\text{det}$) of a $3 \times 3$ matrix, I can use the following formula:

$$
\Large
\text{det} \begin{bmatrix} a & b & c \\ d & e & f \\ g & h & i \end{bmatrix} = a \times \text{det} \begin{bmatrix} e & f \\ h & i \end{bmatrix} - b \times \text{det} \begin{bmatrix} d & f \\ g & i \end{bmatrix} + c \times \text{det} \begin{bmatrix} d & e \\ g & h \end{bmatrix}
$$

The terms are added/ subtracted depending on the sum the row and column indices of the coefficient. Even = positive. Odd = negative. For example "- b" is negative because b is $x_{12}$, thus the sum of the row and column indices $= 1+2 = 3 = odd$.

Applying this to the $3 \times 3$ matrix:

$$
\Large
A = \begin{bmatrix}
1 & 1 & 1 \\
1 & 2 & 4 \\
1 & 4 & 10 
\end{bmatrix}
=
\begin{bmatrix}
a & b & c \\
d & e & f \\
g & h & i 
\end{bmatrix}
$$

Thus,

$$
\Large
\text{det}(A) =
1 \times \text{det} \begin{bmatrix} 2 & 4 \\ 4 & 10 \end{bmatrix} - 1 \times \text{det} \begin{bmatrix} 1 & 4 \\ 1 & 10 \end{bmatrix} + 1 \times \text{det} \begin{bmatrix} 1 & 2 \\ 1 & 4 \end{bmatrix}
$$

To calculate the determinants of the $2 \times 2$ matrices, I can apply the formula:

$$
\Large
\text{det}\begin{bmatrix} a & b \\ c & d \end{bmatrix} = ad - bc
$$

Corresponding calculations for determinants of the $2 \times 2$ matrices:

$$
\Large
\text{det} 
 \begin{bmatrix} 2 & 4 \\ 4 & 10 \end{bmatrix} = 20 - 16 =\mathbf{4}
$$

$$
\Large
 \text{det} \begin{bmatrix} 1 & 4 \\ 1 & 10 \end{bmatrix} = 10 - 4 =\mathbf{6}
$$

$$
\Large
 \text{det} \begin{bmatrix} 1 & 2 \\ 1 & 4 \end{bmatrix} = 4-2 = \mathbf{2}
$$

Substituting these determinants into the aforementioned $3 \times 3$ matrix determinant formula gives:

$$
\Large
\text{det}(A)=
(1 \times 4)  - (1 \times 6) + (1 \times 2) = \mathbf{0}
$$

The determinant of the coefficient matrix and therefore the matrix vector is $0$. This means that the matrix is singular:

$$
\Large
\text{det}(A) = Ax = 0
$$\

In being singular, the matrix does not have an inverse meaning that the corresponding linear equations have either no, or unlimited solutions. Regardless, in this case, the initial equations have no unique solution for $η$.

\
\
\
\

## Question 1b

#### The Set of Linear Equations:

$$
\Large
\begin{aligned}
x + y + z &= 1 \\
x + 2y + 4z &= \eta \\
x + 4y + 10z &= \eta^2
\end{aligned}
$$\

#### The Corresponding Augmented Matrix:

$$
\Large
\left[
\begin{array}{ccc|c}
1 & 1 & 1 & 1 \\
1 & 2 & 4 & \eta \\
1 & 4 & 10 & \eta^2
\end{array}
\right]
$$

#### Using Gaussian Elimination to Identify the Solutions for η:

1.  $$\Large
    (\text{Row }2) -1 \times (1) = 
    \left[
    \begin{array}{ccc|c}
    1 & 1 & 1 & 1 \\
    0 & 1 & 3 & \eta-1 \\
    1 & 4 & 10 & \eta^2
    \end{array}
    \right]
    $$

2.  $$\Large
     (\text{Row } 3) -1 \times (1) = 
    \left[
    \begin{array}{ccc|c}
    1 & 1 & 1 & 1 \\
    0 & 1 & 3 & \eta-1 \\
    0 & 3 & 9 & \eta^2-1
    \end{array}
    \right]
    $$

3.  $$\Large
    (\text{Row }3) -3 \times (2) = 
    \left[
    \begin{array}{ccc|c}
    1 & 1 & 1 & 1 \\
    0 & 1 & 3 & \eta-1 \\
    0 & 0 & 0 & \eta^2-3\eta +2
    \end{array}
    \right]
    $$\

The equation corresponding to the bottom row of the augmented matrix is 0. It is also a quadratic equation where:

$$
\Large
\eta^2-3\eta +2
= ax^2 + bx + c = 0
$$

Now, I can either factorise:

$$
\Large
\eta^2-3\eta +2=
(\eta-1)(\eta-2)= 0
$$

Or alternatively, I can use the quadratic formula to derive the solutions for $\eta$:

$$
\Large
\eta = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}
 = \frac{-(-3) \pm \sqrt{-3^2 - 4\times1\times2}}{2\times1}
 = 1 \text{ or } 2
$$

\

$$
\Large
\eta = 1 \text{ or } 2
$$

\

#### Characterising the equations for both solutions:

##### Where $\eta = 1$:

$$
\Large
\begin{aligned}
x + y + z &= 1 \\
x + 2y + 4z &= 1 \\
x + 4y + 10z &= 1^2
\end{aligned}
$$

Corresponding augmented matrix:

$$
\Large
\left[
\begin{array}{ccc|c}
1 & 1 & 1 & 1 \\
0 & 1 & 3 & 1-1 \\
0 & 0 & 0 & 1^2-3(1)\ + 2
\end{array}
\right]
\Large
= 
\left[
\begin{array}{ccc|c}
1 & 1 & 1 & 1 \\
0 & 1 & 3 & 0 \\
0 & 0 & 0 & 0\ 
\end{array}
\right]
$$

Thus, when $\eta = 1$:

$$
\Large
\begin{aligned}
x + y + z &= 1 \\
y + 3z & = 0 \\
\end{aligned}
$$

## ?Now I can assign the free variable (z) to a parameter and characterise ?

\
\
\

##### Where $\eta = 2$:

$$
\Large
\begin{aligned}
x + y + z &= 1 \\
x + 2y + 4z &= 2 \\
x + 4y + 10z &= 2^2
\end{aligned}
$$

Corresponding augmented matrix:

$$
\Large
\left[
\begin{array}{ccc|c}
1 & 1 & 1 & 1 \\
0 & 1 & 3 & 2-1 \\
0 & 0 & 0 & 2^2-3(2)\ + 2
\end{array}
\right]
\Large
= 
\left[
\begin{array}{ccc|c}
1 & 1 & 1 & 1 \\
0 & 1 & 3 & 1 \\
0 & 0 & 0 & 0\ 
\end{array}
\right]
$$

Thus when $\eta = 2$:

$$
\Large
\begin{aligned}
x + y + z &= 1 \\
y + 3z & = 1 \\
\end{aligned}
$$

## ?Now I can assign the free variable (z) to a parameter and characterise?

\
\
\
\
\
\

## Question 2a

$P_{1} = P(\text{Alice winning when first to throw})$

The distribution shares some characteristics of the Bernoulli distribution whereby there are two outcomes (success & failure) and discrete. However, the question is asking for the probability to a given success. The balls are thrown (trials) until a success (hit). Furthermore, I have to assume that each throw is independent of the other which makes the distribution memoryless with two outcomes: success (win), & failure (miss). This is a geometric distribution.

$$
\large
\left.
\begin{aligned}
\text{Hit} = α\\
\text{Miss}= 1-α\
\end{aligned}
\right\} \text{Alice}
$$

$$
\large
\left.
\begin{aligned}
\text{Hit} =  β\\
\text{Miss}= 1- β\
\end{aligned}
\right\} \text{Ben}
$$

#### The probability of Alice missing Ben with her first throw and going on to win:

P(Alice misses first throw) = $1 - α$

P(Ben misses first throw) = $1-β$

Thus,\
P(Alice miss, Ben miss) = $(1 - α)\times (1-β)$

So,\
P(Alice miss, Ben miss, Alice goes onto win) = $P_{1}\times(1 - α) \times(1 - β)$

$P_{1}$ is able to be used here because of the memoryless nature of the geometric distribution; no matter how many trials have transpired prior to a given throw of Alice's, the probability that she will win from that point onwards is still $P_{1}$.

\
\

## Question 2b

Because I'm working with the Geometric distribution with parameter p, X has a probability mass function:

$$
\large
P(X = k) = p(k) =
\left\{
\begin{aligned}
(1 − p)^{k-1}p,\\
0\
\end{aligned}
\right.
$$

This is not conditional probability as the probability is with respect to the beginning of the round.

As discussed, the probability that Alice and Ben both miss in a given round is: $(1 - α)\times (1-β)$

\
This could repeat n times where n can denotes any integer (theoretically): $((1 - α)\times (1-β))^n$ meaning the probability will be a summation of all the rounds where n is potentially indefinite $(\sum_{n=1}^{\infty})$

\
Eventual success from Alice (given that she throws first) therefore gives: $$
\Large
P(X = n) =  \sum_{n=1}^{\infty} ((1 - α)\times (1-β))^{n-1} \timesα
$$\

The $n-1$ term is used because Alice and Ben must miss every time until the final round (hence $-1$); the $n$-th round is the one she wins (success).

The $α$ term is therefore added as it still denotes the probability that she will hit successfully.

\
Therefore, substituting my terms in gives:

$$
\Large
P_{1} =  \sum_{n=1}^{\infty} ((1-α)\times(1-β))^{n-1}\times α
$$

In line with the geometric series, this is then equivalent to: $$
\Large
P_{1} =
α\times \sum_{n=0}^{\infty} ((1-α)\times(1-β))^{n}
$$

This holds even if n = 0 because in this instance the equation would still equate to α.\
\

In the infinite geometric series equation $\sum_{n=0}^{\infty}r^n=\frac{1}{1-r}$ where $|r|< 1$\
$(1-α)\times(1-β)$ is the common ratio (this holds because the terms are probabilities and so \< 1) Thus:

$$
\Large 
r = (1-α)\times(1-β)
$$\
\

Finally, substituting the values into the geometric series equation gives: $$
\Large
P_{1} = α \times \frac{1}{1-(1-α)\times(1-β)}
$$

## Question 2c

For Ben to throw go first but Alice still to win, there must be $1-β$ term.

However given the order of throws, the point at which Alice wins must be preceded by a miss from Ben $(1-β)$ after which she can throw (and win). Therefore,

$$
\Large 
P_{2}\neq \sum_{n=1}^{\infty} ((1-β) \times (1-α))^{n-1} \times α
$$

Instead there must be a final $1-β$ before Alice is victorious. Again, the $n$-th round is the one where she wins (also the round where Ben misses a final time) Thus,

$$
\Large 
P_{2} = \sum_{n=1}^{\infty} ((1-β) \times (1-α))^{n-1} \times (1-β) \times α 
$$

By applying the geometric series steps as in the previous question, I get that: $$
\Large 
P_{2} = (1-β) \times α \times \sum_{n=0}^{\infty} ((1-β) \times (1-α))^{n}
$$

And finally:

$$
\Large
P_{2} = (1-β)\timesα \times \frac{1}{1-(1-α)\times(1-β)}
$$\
\
\
\

## Question 3a

Infinite geometric series for $|r| < 1$:\
$$
\Large 
\sum_{n=0}^{\infty}r^n=\frac{1}{1-r} 
$$\

The equation $\sum_{n=0}^{\infty}n(n-1)r^{n-2}$ has no common ratio i.e. there are extra terms: $n(n-1)$ - a polynomial.This means it is not part of the geometric series. However, I can but I can use properties of the geometric series to work out the sum of the latter. I can do so by altering the geometric series to take the same form of the equation of interest.

.Differentiating the geometric series gives a similar form to the second expression:

$$
\Large
\frac{d}{dx} \left( \sum_{n=0}^{\infty} r^n \right) = \frac{d}{dx} \left( \frac{1}{1 - r} \right)
$$

For the first term (equation) I apply the power rule: $\frac{d}{dx} \left( x^n \right) = n x^{n-1}$ Note, I am derivating with respect to $r$ i.e. $r = x$ to match geometric series form.

$$
\Large
\frac{d}{dx} \left( \sum_{n=0}^{\infty} r^n \right) =
\sum_{n=0}^{\infty} nr^{n-1}
$$

And for the sum I can apply the quotient rule to find the derivative: $\frac{d}{dx} \left( \frac{f}{g} \right) = \frac{g \frac{d}{dx}(f) - f \frac{d}{dx}(g)}{g^2}$ Again, I am derivating with respect to r (r = x)

$$
\Large
 \frac{d}{dx} \left( \frac{1}{1 - r} \right) = \frac{(1-r)\times\frac{d}{dx}(1)-1 \times \frac{d}{dx}(1-r)}{(1-r)^2}
 = \frac{1}{(1-r)^2}
$$

Now I have:

$$
\Large
\sum_{n=0}^{\infty} nr^{n-1} = \frac{1}{(1-r)^2}
$$

The equation form now mirrors that of the equation of interest: $\sum_{n=0}^{\infty}n(n-1)r^{n-2}$

#### Next, I can take the second derivative:

The second derivative of $\sum_{n=0}^{\infty}r^n$:

$$
\Large \frac{d^2}{dx^2} \left( \sum_{n=0}^{\infty} r^n \right) = \frac{d}{dx} \sum_{n=0}^{\infty} n r^{n-1}
$$

Using the power rule again:

$$
\Large
\frac{d}{dx}\sum_{n=0}^{\infty} nr^{n-1}
= \sum_{n=0}^{\infty}n(n-1)r^{n-2}
$$

This is the exact same as the equation of interest. Therefore, the second derivative of the given geometric series $( \sum_{n=0}^{\infty} r^n)$ is equal to the equation of interest:

$$
\Large
\frac{d2}{dx2} \left( \sum_{n=0}^{\infty} r^n \right) = \frac{d}{dx}\sum_{n=0}^{\infty} nr^{n-1} = \sum_{n=0}^{\infty}n(n-1)r^{n-2}
$$

Therefore, to find its respective sum, I must find the second derivative of the initial equations sum too. I can do this again with the quotient rule.

$$
\Large
\frac{d2}{dx2} \left( \frac{1}{1 - r} \right) =
\frac{d}{dx} \left( \frac{1}{(1 - r^2)} \right) =
\frac{(1-r)^2\times\frac{d}{dx}(1)-1 \times \frac{d}{dx}(1-r)^2}{((1-r)^2)^2}
$$\
\

Note: I now also have to apply chain rule to differentiate $(1-r)^2$. $1-r$ is the inner function, $^2$ is the outer. So $\frac{d}{dx}(1-r)^2 = -2(1-r)$ Thus, I arrive at:

\
$$
\Large
= \frac{(1-r)^2\times0-1 \times (-2\times(1-r))}{(1-r)^4}=\frac{2(1-r)}{(1-r)^4} =\frac{2}{(1-r)^3}
$$

To conclude:

$$
\Large
\sum_{n=0}^{\infty}n(n-1)r^{n-2} =\frac{2}{(1-r)^3}
$$

\
\
\
\
\

\

\
\
\

\
\
\

## Question 3b

### A:

I need to calculate the expectation of a fine (true when weight $<$ 420.75) per box. This is equivalent to the CDF $F(x) = P(X ≤ 420.75)$. Then I can calculate the expected value of the fine/box.

The tins weight follows a normal distribution $X\text~N(μ = 426, σ = 21)$. The PDF of the normal distribution is:

$$
\Large
f(x \mid \mu, \sigma^2) = \frac{1}{2 \pi \sigma^2} \exp \left\{ -\frac{1}{2 \sigma^2} (x - \mu)^2 \right\}
$$

Substituting the parameters gives:

$$
\Large
f(x \mid 426, 21^2)  = \frac{1}{2 \pi (21)^2} \exp \left\{ -\frac{1}{2 \times (21)^2} (x - 426)^2 \right\}
$$

The corresponding CDF cannot be denoted analytically due to but must be evaluated. The CDF $F(x) = P(X ≤ 420.75)$ is the integral of the PDF. However, the CDF of a normal distribution (denoted as $\Phi$) which means I can consult a 'Z-' probability table. To calculate the CDF corresponding z-value I can use the formula:

$$
\Large
Z = \frac{X - \mu}{\sigma_{\text{sample}}}
$$

Importantly, the variance value in this term is actually standard deviation of a box because it is tied to the sample size. In this case, the sample size is a whole box (100 cans), not a single tin:

$$
\Large
\sigma_{\text{sample}} = \frac{\sigma}{\sqrt{n}} = \frac{21}{\sqrt{100}} = 2.1
$$

This makes sense because as sample size increases, variance should decrease. In turn, by weighing a whole box, the variance of the box will decrease significantly, even if the variance of the individual tins is constant.\

Substituting the parameters into the penultimate equation gives:

$$
\Large
Z = \frac{420.75 - 426}{2.1} = -2.50
$$\

#### Now, I can consult the z-table:

![image](images\ztable.png)\

The CDF's corresponding z-value is 0.0062. This means the probability of a box's weight/can being \<420.75g is 0.62%. To work out the expectation of the fine:

$$
\Large
E(\text{Fine}) = 3000 \times 0.0062 = 18.6
$$

Thus for A:

$$
\Large
E(\text{Fine})=£18.60\text{/box}
$$\
\
\
\
\

### B:

I need to calculate the CDF $F(x) = P(X ≤ 421.8)$. I will use the same logic as before and calculate the z-value before consulting the table. This time, $X = 421.8$ and $n = 25$ which means that to calculate the z-value: $$
\Large
\sigma_{\text{sample}} = \frac{\sigma}{\sqrt{n}} = \frac{21}{\sqrt{25}} = 4.2
$$

This higher $\sigma$ compared to the previous question is to be expected given the smaller sample size. Thus:

$$
\Large
Z = \frac{421.8 - 426}{4.2} = -1
$$\

![image](images\ztableb.png)\

The CDF's corresponding z-value is 0.1587. This means the probability of a box's weight/can being \<421.8g is 15.87%. To work out the expectation of the fine:

$$
\Large
E(\text{Fine}) = 120 \times 0.1587 = 19.044
$$

Thus for B:

$$
\Large
E(\text{Fine})=£19.04\text{/box}
$$\
\
\
\
\

### C:

The probability that X \> 426 is equivalent to $1- P(X ≤ 426)$. I could do this again and consult the z-table but this is unnecessary. 426 is the mean and normally distributed so there is a 0.5 chance that a randomly sampled can will weigh more and 0.5 that it will weigh less. Note: this takes into account the fact that a PDF of a given continuous variable (in this case 426.0g) = 0.

Here, each sampling event is either success $(X > 426)$ or failure $(X ≤ 426)$ where n could take any number. This is characteristic of the geometric formula. As discussed, the finite geometric series is:

$$
\Large 
\sum_{n=0}^{\infty}r^n=\frac{1}{1-r} 
$$\

Since the probability of success = 0.5:

$$
\Large
\sum_{n=0}^{\infty}r^n=\frac{1}{1-0.5}  = 2
$$

As expected, it takes two trials until a success is expected. In turn:

$$
\Large
E(n) = 2
$$

N itself however will have its own distribution which must be considered before plugging into the fine equation. The variance of n can be calculated using the varience formula for geometric distributions:

$$
\Large
Var(n) = \frac{1-p}{p^2}
$$

Thus,

$$
\Large
Var(n) = \frac{1-0.5}{0.5^2}= 2
$$\

To calculate the expectation of the fine I need to use the formula,

$$
\Large
E(n^2) = \text{Var}(n) + (E(n))^2
$$

I have the variance and expected no. samples until success but i also need the expectation of n\^2 because $n(n−1)=n^2−n$. Therefore to calculate the expectation of $n^2$:

$$
\Large
E(n)^2 = 4 + 2^2 = 6
$$

Now I can calculate the E(n(n-1)):

$$
\Large
E(n(n-1))=  E(n^2) - E(n) = 6-2 = 4
$$

Now I have $E(n(n-1))$, I can plug it into the equation:

$$
\Large
E(\text{Fine}) = 5\times{}E(n(n-1))=  5 \times 4 = 20
$$

In conclusion:

$$
\Large
E(\text{Fine}) =  £20/\text{box}
$$

\
\
\
\

## Question 4a

#### Introducing the random variables:

Individually, $X_i^{(1)}$ and $X_i^{(2)}$ random variables follow Bernoulli distributions because its success/failure (outcomes sum to 1), each trial is independent and $n$ is 1.

$$
\Large
X_i^{(1)} = \left\{
\begin{array}{ll}
1, & \text{if B supporter number } i \text{ still supports B at the end of day 1}, \\
0, & \text{if B supporter number } i \text{ changes to an M supporter at the end of day 1}.
\end{array}
\right.
$$

For $i =1...,175$ this is equivalent to:

$$
\Large
X_i^{(1)} = \left\{
\begin{array}{ll}
k= 1, \text{ }p = 1- 0.004 = 0.996 \\
k= 0, \text{ }1-p = 1- 0.996 = 0.004
\end{array}
\right.
$$

In summary, each random variables can be denoted as $X_i^{(1)} \sim \text{Bernoulli}(1,p)\text{ for } i = 1...,175$.

\

For $X^{(2)}$:

$$
\Large
X_i^{(2)} = \left\{
\begin{array}{ll}
1, & \text{if M supporter number } i \text{ changes to a B supporter at the end of day 1}, \\
0, & \text{if M supporter number } i \text{ still supports M at the end of day 1}.
\end{array}
\right.
$$

for $i =1...,186$, this is equivalent to:

$$
\Large
X_i^{(2)} = \left\{
\begin{array}{ll}
k=1, \text{ }p = 0.005 \\
k=0, \text{ }1-p = 1- 0.005 = 0.996
\end{array}
\right.
$$

Again, each random variable trial can be denoted as $X_i^{(2)} \sim \text{Bernoulli}(1,p) \text{ for } i = 1...,186$.\

\
\

#### Expressing the number of B supporters at the end of the first day:

Every trial is independent and peoples allegiance has no bearing on other people. Crucially, distinct Bernoulli trials are repeated; not the same Bernoulli trial. This is because every $X$ is unique within a given day i.e. unique random variables (prospective voters). This precludes the use of the Binomial distribution characteristics; using binomial formulas would imply that the Bernoulli trials are repeated on single person n within a single day. This is not the case.

The number of B supporters at the end of day 1 will be the B's Inital supporters - those that switch allegiance to support M + those that switch allegiance to support B:

$$
\text{B supporters after 1 day} =\text{B  supporters at the start of day} - \text{Supporters who switch to M} + \text{Supporters who switch to B}
$$\
\

Here I will denote 'B supporters at the start of the day' as 'B' and M supporters at the start of the day as 'M' which are both integers. Using the introduced Random distributions this gives:

$$
\Large
\text{B supporters after 1 day} = B -  (B-\sum_{i=1}^{B} X_i^{(1)}) +  \sum_{i=1}^{M}X_i^{(2)}
$$\

The $(B-\sum_{i=1}^{B} X_i^{(1)})$ is included as it is all the failed trials for $X_i^{(1)}$ minus the number of B supporters who switch. The sum symbol is sufficient here because if a trial ends in success,the output is 1. This represents a person so the sum of these will equate to the number of successes. Therefore, probabilities such as $'p'$ and $'p-1'$ do not need to be expressed in the equation above.\
\
\

#### Expected number of B supporters at the end of the first day:

Using the formula, I can calculate the E(B supporters after one day). I must first calculate the sum of the expected occurrences in the two $X_i^{(1)}$ and $X_i^{(2)}$ terms.

I can use the properties of the expectation to calculate these. Because I'm not working with a binomial distribution I will not use its expectation formula. Instead, I will use the expectation formula for the Bernoulli: E(X) = p. Therefore p can be multiplied for the number of Bernoulli trials in each term:

expectations For $E(\sum_{i=1}^{175} X_i^{(1)}) = 175 \times  0.996 = 174.3 = 174$

For $E(\sum_{i=1}^{186}X_i^{(2)}) = 186 \times 0.005 = 0.93 = 1$\

I have rounded these numbers to the nearest integer as people can cast either 1 or 0 votes.

Our terms are:

-   $B = 175$\

-   $M = 186$\

-   $E(\sum_{i=1}^{175} X_i^{(1)}) = 174$\

-   $E(\sum_{i=1}^{186}X_i^{(2)})  = 1$\
    \

Substituting these values gives:\

$$
\Large
\text{B supporters after 1 day} = 175 -  (175-174) +  1 
$$\

Thus:\
$$
\Large
\text{B supporters after 1 day} = 175
$$\
\
\
\
\

## Question 4b

#### Number of M supporters at end of first day:

$$
\text{M supporters after 1 day} =\text{M  supporters at the start of day} + \text{Supporters who switch to M} - \text{Supporters who switch to B}
$$\

Therefore:\
$$
\Large
\text{M supporters after 1 day} = M +  (B-\sum_{i=1}^{B} X_i^{(1)}) -  \sum_{i=1}^{M}X_i^{(2)}
$$\

Substituting the same values from Question 4a gives:\

$$
\Large
\text{M supporters after 1 day} = 186 +  (175-174) - 1 
$$\

Thus:\
$$
\Large
\text{M supporters after 1 day} = 186
$$\
\
\
\
\

## Question 4c

```{r}
# Creating the loops for one day first:

# For the X1 outcomes ----

X1 <- 175  # This will become no. permutations for X1
X1_trials <- numeric(175)  # Where outcomes of each Bernoulli trial will be stored
X1_probs <- c(0.996, 0.004)  # The two probabilities for each X2 trial
results <- c(1, 0) # The corresponding outputs for the probabilities above 
no._X1_successes <- 0  # Initial number of successes (Will grow with loops)


for(i in 1:X1){  # Sampling everyone
  X1_trials[i] <- sample(results, 
                      size = 1, # 1 Bernoulli per person
                      replace = F,  # Only one decision per person (all people must be sampled)
                      prob = X1_probs)  # Pre-specified probabilities
  if (X1_trials[i] == 1) {  # 'If current trial is a success...'
    no._X1_successes <- no._X1_successes + 1  # '...Increase the number of success by 1
  }
}

print(no._X1_successes)




# For X2 outcomes ----

X2 <- 186  # This will become no. permutations for X1
X2_trials <- numeric(186)  # Where outcomes of each Bernoulli trial will be stored
X2_probs <- c(0.005, 0.996)  # The two probabilities for each X2 trial
results <- c(1, 0) # The corresponding outputs for the probabilities above 
no._X2_successes <- 0  # Initial number of successes (Will grow with loops)


for(i in 1:X2){  # Sampling everyone
  X2_trials[i] <- sample(results, 
                      size = 1, # 1 Bernoulli per person
                      replace = F,  # Only one decision per person (all people must be sampled)
                      prob = X2_probs)  # Pre-specificed probabilities
  if (X2_trials[i] == 1) {  # 'If current trial is a success...'
    no._X2_successes <- no._X2_successes + 1  # '...Increase the number of success by 1
  }
}


print(no._X2_successes)

No._B <- X1 - (X1 - no._X1_successes) + no._X2_successes  # No. B supporters after Day 1
No._M <- 361 - No._B  # No. M supporters after Day 1









# Now I need to have this iterated 14 times but have 'No._B' and 'No._M' values
# assigned to the 'X1' and 'X1_trials' and 'X2' and 'X2_trials' respectively 

# For the 14 days:
no._iterations <- 14  # No. days 


X1 <- 175  # This will become no. permutations for X1
X1_trials <- numeric(175)  # Where outcomes of each Bernoulli trial will be stored
no._X1_successes <- 0  # Initial number of successes (Will grow with loops)

X2 <- 186  # This will become no. permutations for X1
X2_trials <- numeric(186)  # Where outcomes of each Bernoulli trial will be stored
no._X2_successes <- 0  # Initial number of successes (Will grow with loops)



for (day in 1:no._iterations) { # Loop for each day (n = 14)
  X1_trials <- numeric(X1)  # Store outcomes for X1 trials
  no._X1_successes <- 0  # Reset success count for X1
  
  for (i in 1:X1) {
    X1_trials[i] <- sample(results, size = 1, replace = FALSE, prob = X1_probs)
    if (X1_trials[i] == 1) {
      no._X1_successes <- no._X1_successes + 1
    }
  }
  
  # For X2 outcomes
  X2_trials <- numeric(X2)  # Store outcomes for X2 trials
  no._X2_successes <- 0  # Reset success count for X2
  
  for (i in 1:X2) {
    X2_trials[i] <- sample(results, size = 1, replace = FALSE, prob = X2_probs)
    if (X2_trials[i] == 1) {
      no._X2_successes <- no._X2_successes + 1
    }
  }
  
  # Calculate No._B and No._M
  No._B <- X1 - (X1 - no._X1_successes) + no._X2_successes
  No._M <- 361 - No._B
  
  # Update X1 and X2 for the next iteration
  X1 <- No._B
  X2 <- No._M
}

print(X1)  # No. B supporters after 14 days
print(X2)  # No M supporters after 14 days














# Now to work out the probability that No._B > No._M, I will nest one more loop
# I will repeat the 14 days experiment thousands of times and 
# calculate what proportion of the outputs have No._B > No._M


# Final loop iterations ----
simulations <- 10000  # This sample size willallow the true probability to be inferred confidently
B_more_than_M <- 0  # Intial No. where B > M


X1 <- 175  # This will become no. permutations for X1
X1_trials <- numeric(175)  # Where outcomes of each Bernoulli trial will be stored
no._X1_successes <- 0  # Initial number of successes (Will grow with loops)

X2 <- 186  # This will become no. permutations for X1
X2_trials <- numeric(186)  # Where outcomes of each Bernoulli trial will be stored
no._X2_successes <- 0  # Initial number of successes (Will grow with loops)



# Run the simulation 10000 times
for (sim in 1:simulations) {
  
  # Run the 14-iteration loop
  for (day in 1:no._iterations) {
    # For X1 outcomes
    X1_trials <- numeric(X1)
    no._X1_successes <- 0
    
    for (i in 1:X1) {
      X1_trials[i] <- sample(results, size = 1, replace = FALSE, prob = X1_probs)
      if (X1_trials[i] == 1) {
        no._X1_successes <- no._X1_successes + 1
      }
    }
    
    # For X2 outcomes
    X2_trials <- numeric(X2)
    no._X2_successes <- 0
    
    for (i in 1:X2) {
      X2_trials[i] <- sample(results, size = 1, replace = FALSE, prob = X2_probs)
      if (X2_trials[i] == 1) {
        no._X2_successes <- no._X2_successes + 1
      }
    }
    
    # Calculate No._B and No._M
    No._B <- X1 - (X1 - no._X1_successes) + no._X2_successes
    No._M <- 361 - No._B
    
    # Update X1 and X2 for the next iteration
    X1 <- No._B
    X2 <- No._M
  }
  
  # Check if No._B > No._M in the final result and count it
  if (No._B > No._M) {
    B_more_than_M <- B_more_than_M + 1
  }
}

answer <- B_more_than_M/ 10000  # Average number of times B has the majority

print(answer)  # = 0.098


```

## Question 4d

```{r}
# Repeat but change no. iterations to 60
no._iterations_60 <- 60  # No. days 

# Final loop iterations ----
simulations <- 10000  # This sample size willallow the true probability to be inferred confidently
B_more_than_M <- 0  # Intial No. where B > M


X1 <- 175  # This will become no. permutations for X1
X1_trials <- numeric(175)  # Where outcomes of each Bernoulli trial will be stored
no._X1_successes <- 0  # Initial number of successes (Will grow with loops)

X2 <- 186  # This will become no. permutations for X1
X2_trials <- numeric(186)  # Where outcomes of each Bernoulli trial will be stored
no._X2_successes <- 0  # Initial number of successes (Will grow with loops)



# Run the simulation 10000 times
for (sim in 1:simulations) {
  
  # Run the 14-iteration loop
  for (day in 1:no._iterations_60) {
    # For X1 outcomes
    X1_trials <- numeric(X1)
    no._X1_successes <- 0
    
    for (i in 1:X1) {
      X1_trials[i] <- sample(results, size = 1, replace = FALSE, prob = X1_probs)
      if (X1_trials[i] == 1) {
        no._X1_successes <- no._X1_successes + 1
      }
    }
    
    # For X2 outcomes
    X2_trials <- numeric(X2)
    no._X2_successes <- 0
    
    for (i in 1:X2) {
      X2_trials[i] <- sample(results, size = 1, replace = FALSE, prob = X2_probs)
      if (X2_trials[i] == 1) {
        no._X2_successes <- no._X2_successes + 1
      }
    }
    
    # Calculate No._B and No._M
    No._B <- X1 - (X1 - no._X1_successes) + no._X2_successes
    No._M <- 361 - No._B
    
    # Update X1 and X2 for the next iteration
    X1 <- No._B
    X2 <- No._M
  }
  
  # Check if No._B > No._M in the final result and count it
  if (No._B > No._M) {
    B_more_than_M <- B_more_than_M + 1
  }
}

answer <- B_more_than_M/ 10000  # Average number of times B has the majority

print(answer)  # = 0.96

```

#### Conclusion 

P(B>M after 60 days) = 0.989 = 98.9%.

The difference in probability of B winning with the delay is:

$$
\Large
0.989 - 0.098 = 0.891 
$$
Therefore the increase in B probability of winning following the delay is:
$$
\Large
= +89.1\text%
$$

FIND ANSWER INCREASE SIMS GRADUALLY

### \## References

Linear Algebra Slides, 2020
